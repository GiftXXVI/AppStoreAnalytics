{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pygments import highlight\n",
    "from pygments.lexers import JsonLexer\n",
    "from pygments.formatters import TerminalFormatter\n",
    "\n",
    "from google_play_scraper import Sort, reviews, app\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
    "\n",
    "app_packages = [\n",
    "    'com.google.android.apps.meetings',\n",
    "    'com.instagram.android',\n",
    "    'com.microsoft.teams',\n",
    "    'com.zhiliaoapp.musically',\n",
    "    'com.whatsapp',\n",
    "    'us.zoom.videomeetings'\n",
    "]\n",
    "\n",
    "app_infos = []\n",
    "\n",
    "for ap in tqdm(app_packages):\n",
    "  info = app(ap, lang='en', country='us')\n",
    "  del info['comments']\n",
    "  app_infos.append(info)    \n",
    "\n",
    "app_infos_df = pd.DataFrame(app_infos)\n",
    "app_infos_df.to_csv('apps.csv', index=None, header=True)\n",
    "\n",
    "app_reviews = []\n",
    "\n",
    "for ap in tqdm(app_packages):\n",
    "  for score in list(range(1, 6)):\n",
    "  #for x in list(range(1, 10)):\n",
    "    for sort_order in [Sort.NEWEST]:\n",
    "      rvs, _ = reviews(\n",
    "        ap,\n",
    "        lang='en',\n",
    "        country='us',\n",
    "        sort=sort_order,\n",
    "        count= 10000,\n",
    "        filter_score_with=score\n",
    "        #filter_score_with=None\n",
    "      )\n",
    "      for r in rvs:\n",
    "        r['sortOrder'] = 'most_relevant' if sort_order == Sort.MOST_RELEVANT else 'newest'\n",
    "        r['appId'] = ap\n",
    "      app_reviews.extend(rvs)\n",
    "\n",
    "app_reviews_df = pd.DataFrame(app_reviews)\n",
    "app_reviews_df.to_csv('reviews.csv', index=None, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TF-IDF Model:\n",
      "\n",
      "Topic: 0 \n",
      "Words: 0.024*\"accountNN\" + 0.021*\"famousJJ\" + 0.020*\"timeNN_passNN\" + 0.015*\"updateNN\" + 0.014*\"contentNN\" + 0.013*\"beautifulNN\" + 0.011*\"enjoyableJJ\" + 0.010*\"slowVB\" + 0.010*\"funnyJJ_videosNNS\" + 0.008*\"terribleJJ\"\n",
      "\n",
      "Topic: 1 \n",
      "Words: 0.031*\"entertainmentNN\" + 0.025*\"timeNN\" + 0.021*\"starNN\" + 0.014*\"addictiveJJ\" + 0.012*\"friendsNNS\" + 0.011*\"addictedVBN\" + 0.008*\"jobNN\" + 0.008*\"musicallyRB\" + 0.007*\"optionNN\" + 0.007*\"danceNN\"\n",
      "\n",
      "Topic: 2 \n",
      "Words: 0.044*\"entertainingVBG\" + 0.022*\"trashNN\" + 0.016*\"watchNN\" + 0.016*\"interestingVBG\" + 0.013*\"wasteNN_timeNN\" + 0.012*\"viralJJ\" + 0.012*\"annoyingVBG\" + 0.009*\"bitNN\" + 0.008*\"reasonNN\" + 0.007*\"talentNN\"\n",
      "\n",
      "Topic: 3 \n",
      "Words: 0.033*\"videoNN\" + 0.032*\"peopleNNS\" + 0.017*\"workingVBG\" + 0.016*\"addictingVBG\" + 0.016*\"toxicNN\" + 0.014*\"problemNN\" + 0.013*\"dataNNS\" + 0.013*\"gameNN\" + 0.010*\"stupidJJ\" + 0.010*\"problemsNNS\"\n",
      "\n",
      "Topic: 4 \n",
      "Words: 0.027*\"followersNNS\" + 0.022*\"starsNNS\" + 0.019*\"workNN\" + 0.013*\"easyJJ\" + 0.012*\"likesNNS\" + 0.009*\"postNN\" + 0.009*\"adsNNS\" + 0.008*\"platformNN\" + 0.008*\"deleteNN\" + 0.008*\"freeJJ\"\n",
      "\n",
      "Topic: 5 \n",
      "Words: 0.041*\"funnyNN\" + 0.038*\"downloadNN\" + 0.037*\"videosNNS\" + 0.013*\"installNN\" + 0.008*\"phoneNN\" + 0.007*\"lotsNNS\" + 0.007*\"guysNNS\" + 0.007*\"followersNNS_liveVBP\" + 0.007*\"liveVBP\" + 0.007*\"garbageNN\"\n",
      "\n",
      "Topic: 6 \n",
      "Words: 0.028*\"liveJJ\" + 0.020*\"viewsNNS\" + 0.014*\"followVB\" + 0.011*\"viralJJ_videoNN\" + 0.010*\"viewNN\" + 0.010*\"glitchesNNS\" + 0.009*\"supportNN\" + 0.007*\"cancerNN\" + 0.007*\"socialJJ_mediaNNS\" + 0.007*\"takesVBZ_spaceNN\"\n",
      "\n",
      "\n",
      "Perplexity:  -8.72243210784131\n",
      "\n",
      "Coherence Score:  0.6247713170486515\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"reviews_tiktok_v2.csv\")\n",
    "data_text = data[['content']]\n",
    "data_text['index'] = data_text.index\n",
    "documents = data_text\n",
    "\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "my_stopwords = ['enjoyable','terrible','beautiful','option','guess','omg','ali','enjoy','gay','sucks','things','alot','kinda','wo','plz','open','soo','day','meh','eh','tho','ha','life','pretty','kids','stuff','idk','lol','bekar','dont','hate','aap','a','amazing','am','and','application','app','apps','awesome','bad','best','better','bhai','but',\n",
    "'cool','ever','excellent','experience','fabulous','fantastic','far','fine','forIN','fun','fuy','gd','good','google',\n",
    "'great','gud','hai','happy','hi','in','i','instagram','insta','isZ','it','its','just','khan','k','kumar','like',\n",
    "'loved','lovely','love','mast','me','messenger','much','my','nice','no','not','nyc','okay','ok','one','op','osm',\n",
    "'perfect','sometimes','so','supe','super','supper','thank','thanks','the','this','tik','tiktok','tok','to','use','very',\n",
    "'well','whatsapp','whats','what','with','wonderful','worst','wow','you','zoom','is','can','for','be','also','was','now',\n",
    "'are','on','of','as','than','if','because','or','do','some','will','all','us','could','have','an','when','always','more',\n",
    "'using','how','otherwise','we','from','your','you','appp','such','that','too','ap','ne','new','should','thing','there',\n",
    "'android','awsome','lot','get','superb','really','yes','no','outstanding','keep','need','must','he','make','ca','see',\n",
    "'by','at','has','been',\"a\",\"about\",\"above\",\"after\",\"again\",\"against\",\"ain\",\"all\",\"am\",\"an\",\"and\",\"any\",\"are\",\"aren\",\n",
    "\"aren't\",\"as\",\"at\",\"be\",\"because\",\"been\",\"before\",\"being\",\"below\",\"between\",\"both\",\"but\",\"by\",\"can\",\"couldn\",\n",
    "\"couldn't\",\"d\",\"did\",\"didn\",\"didn't\",\"do\",\"does\",\"doesn\",\"doesn't\",\"doing\",\"don\",\"don't\",\"down\",\"during\",\"each\",\"few\",\n",
    "\"for\",\"from\",\"further\",\"had\",\"hadn\",\"hadn't\",\"has\",\"hasn\",\"hasn't\",\"have\",\"haven\",\"haven't\",\"having\",\"he\",\"her\",\"here\",\n",
    "\"hers\",\"herself\",\"him\",\"himself\",\"his\",\"how\",\"i\",\"if\",\"in\",\"into\",\"is\",\"isn\",\"isn't\",\"it\",\"it's\",\"its\",\"itself\",\"just\",\n",
    "\"ll\",\"m\",\"ma\",\"me\",\"mightn\",\"mightn't\",\"more\",\"most\",\"mustn\",\"mustn't\",\"my\",\"myself\",\"needn\",\"needn't\",\"no\",\"nor\",\"not\",\n",
    "\"now\",\"o\",\"of\",\"off\",\"on\",\"once\",\"only\",\"or\",\"other\",\"our\",\"ours\",\"ourselves\",\"out\",\"over\",\"own\",\"re\",\"s\",\"same\",\"shan\",\n",
    "\"shan't\",\"she\",\"she's\",\"should\",\"should've\",\"shouldn\",\"shouldn't\",\"so\",\"some\",\"such\",\"t\",\"than\",\"that\",\"that'll\",\"the\",\n",
    "\"their\",\"theirs\",\"them\",\"themselves\",\"then\",\"there\",\"these\",\"they\",\"this\",\"those\",\"through\",\"to\",\"too\",\"under\",\"until\",\n",
    "\"up\",\"ve\",\"very\",\"was\",\"wasn\",\"wasn't\",\"we\",\"were\",\"weren\",\"weren't\",\"what\",\"when\",\"where\",\"which\",\"while\",\"who\",\"whom\",\n",
    "\"why\",\"will\",\"with\",\"won\",\"won't\",\"wouldn\",\"wouldn't\",\"y\",\"you\",\"you'd\",\"you'll\",\"you're\",\"you've\",\"your\",\"yours\",\n",
    "\"yourself\",\"yourselves\",\"could\",\"he'd\",\"he'll\",\"he's\",\"here's\",\"how's\",\"i'd\",\"i'll\",\"i'm\",\"i've\",\"let's\",\"ought\",\n",
    "\"she'd\",\"she'll\",\"that's\",\"there's\",\"they'd\",\"they'll\",\"they're\",\"they've\",\"we'd\",\"we'll\",\"we're\",\"we've\",\"what's\",\n",
    "\"when's\",\"where's\",\"who's\",\"why's\",\"would\",\"able\",\"abst\",\"accordance\",\"according\",\"accordingly\",\"across\",\"act\",\n",
    "\"actually\",\"added\",\"adj\",\"affected\",\"affecting\",\"affects\",\"afterwards\",\"ah\",\"almost\",\"alone\",\"along\",\"already\",\"also\",\n",
    "\"although\",\"always\",\"among\",\"amongst\",\"announce\",\"another\",\"anybody\",\"anyhow\",\"anymore\",\"anyone\",\"anything\",\"anyway\",\n",
    "\"anyways\",\"anywhere\",\"apparently\",\"approximately\",\"arent\",\"arise\",\"around\",\"aside\",\"ask\",\"asking\",\"auth\",\"available\",\n",
    "\"away\",\"awfully\",\"b\",\"back\",\"became\",\"become\",\"becomes\",\"becoming\",\"beforehand\",\"begin\",\"beginning\",\"beginnings\",\n",
    "\"begins\",\"behind\",\"believe\",\"beside\",\"besides\",\"beyond\",\"biol\",\"brief\",\"briefly\",\"c\",\"ca\",\"came\",\"cannot\",\"can't\",\n",
    "\"cause\",\"causes\",\"certain\",\"certainly\",\"co\",\"com\",\"come\",\"comes\",\"contain\",\"containing\",\"contains\",\"couldnt\",\"date\",\n",
    "\"different\",\"done\",\"downwards\",\"due\",\"e\",\"ed\",\"edu\",\"effect\",\"eg\",\"eight\",\"eighty\",\"either\",\"else\",\"elsewhere\",\"end\",\n",
    "\"ending\",\"enough\",\"especially\",\"et\",\"etc\",\"even\",\"ever\",\"every\",\"everybody\",\"everyone\",\"everything\",\"everywhere\",\"ex\",\n",
    "\"except\",\"f\",\"far\",\"ff\",\"fifth\",\"first\",\"five\",\"fix\",\"followed\",\"following\",\"follows\",\"former\",\"formerly\",\"forth\",\n",
    "\"found\",\"four\",\"furthermore\",\"g\",\"gave\",\"get\",\"gets\",\"getting\",\"give\",\"given\",\"gives\",\"giving\",\"go\",\"goes\",\"gone\",\n",
    "\"got\",\"gotten\",\"h\",\"happens\",\"hardly\",\"hed\",\"hence\",\"hereafter\",\"hereby\",\"herein\",\"heres\",\"hereupon\",\"hes\",\"hi\",\"hid\",\n",
    "\"hither\",\"home\",\"howbeit\",\"however\",\"hundred\",\"id\",\"ie\",\"im\",\"immediate\",\"immediately\",\"importance\",\"important\",\"inc\",\n",
    "\"indeed\",\"index\",\"information\",\"instead\",\"invention\",\"inward\",\"itd\",\"it'll\",\"j\",\"k\",\"keep\",\"keeps\",\"kept\",\"kg\",\"km\",\n",
    "\"know\",\"known\",\"knows\",\"l\",\"largely\",\"last\",\"lately\",\"later\",\"latter\",\"latterly\",\"least\",\"less\",\"lest\",\"let\",\"lets\",\n",
    "\"like\",\"liked\",\"likely\",\"line\",\"little\",\"'ll\",\"look\",\"looking\",\"looks\",\"ltd\",\"made\",\"mainly\",\"make\",\"makes\",\"many\",\n",
    "\"may\",\"maybe\",\"mean\",\"means\",\"meantime\",\"meanwhile\",\"merely\",\"mg\",\"might\",\"million\",\"miss\",\"ml\",\"moreover\",\"mostly\",\n",
    "\"mr\",\"mrs\",\"much\",\"mug\",\"must\",\"n\",\"na\",\"name\",\"namely\",\"nay\",\"nd\",\"near\",\"nearly\",\"necessarily\",\"necessary\",\"need\",\n",
    "\"needs\",\"neither\",\"never\",\"nevertheless\",\"new\",\"next\",\"nine\",\"ninety\",\"nobody\",\"non\",\"none\",\"nonetheless\",\"noone\",\n",
    "\"normally\",\"nos\",\"noted\",\"nothing\",\"nowhere\",\"obtain\",\"obtained\",\"obviously\",\"often\",\"oh\",\"ok\",\"okay\",\"old\",\"omitted\",\n",
    "\"one\",\"ones\",\"onto\",\"ord\",\"others\",\"otherwise\",\"outside\",\"overall\",\"owing\",\"p\",\"page\",\"pages\",\"part\",\"particular\",\n",
    "\"particularly\",\"past\",\"per\",\"perhaps\",\"placed\",\"please\",\"plus\",\"poorly\",\"possible\",\"possibly\",\"potentially\",\"pp\",\n",
    "\"predominantly\",\"present\",\"previously\",\"primarily\",\"probably\",\"promptly\",\"proud\",\"provides\",\"put\",\"q\",\"que\",\"quickly\",\n",
    "\"quite\",\"qv\",\"r\",\"ran\",\"rather\",\"rd\",\"readily\",\"really\",\"recent\",\"recently\",\"ref\",\"refs\",\"regarding\",\"regardless\",\n",
    "\"regards\",\"related\",\"relatively\",\"research\",\"respectively\",\"resulted\",\"resulting\",\"results\",\"right\",\"run\",\"said\",\"saw\",\n",
    "\"say\",\"saying\",\"says\",\"sec\",\"section\",\"see\",\"seeing\",\"seem\",\"seemed\",\"seeming\",\"seems\",\"seen\",\"self\",\"selves\",\"sent\",\n",
    "\"seven\",\"several\",\"shall\",\"shed\",\"shes\",\"show\",\"showed\",\"shown\",\"showns\",\"shows\",\"significant\",\"significantly\",\n",
    "\"similar\",\"similarly\",\"since\",\"six\",\"slightly\",\"somebody\",\"somehow\",\"someone\",\"somethan\",\"something\",\"sometime\",\n",
    "\"sometimes\",\"somewhat\",\"somewhere\",\"soon\",\"sorry\",\"specifically\",\"specified\",\"specify\",\"specifying\",\"still\",\"stop\",\n",
    "\"strongly\",\"sub\",\"substantially\",\"successfully\",\"sufficiently\",\"suggest\",\"sup\",\"sure\",\"take\",\"taken\",\"taking\",\"tell\",\n",
    "\"tends\",\"th\",\"thank\",\"thanks\",\"thanx\",\"thats\",\"that've\",\"thence\",\"thereafter\",\"thereby\",\"thered\",\"therefore\",\"therein\",\n",
    "\"there'll\",\"thereof\",\"therere\",\"theres\",\"thereto\",\"thereupon\",\"there've\",\"theyd\",\"theyre\",\"think\",\"thou\",\"though\",\n",
    "\"thoughh\",\"thousand\",\"throug\",\"throughout\",\"thru\",\"thus\",\"til\",\"tip\",\"together\",\"took\",\"toward\",\"towards\",\"tried\",\n",
    "\"tries\",\"truly\",\"try\",\"trying\",\"ts\",\"twice\",\"two\",\"u\",\"un\",\"unfortunately\",\"unless\",\"unlike\",\"unlikely\",\"unto\",\"upon\",\n",
    "\"ups\",\"us\",\"use\",\"used\",\"useful\",\"usefully\",\"usefulness\",\"uses\",\"using\",\"usually\",\"v\",\"value\",\"various\",\"'ve\",\"via\",\n",
    "\"viz\",\"vol\",\"vols\",\"vs\",\"w\",\"want\",\"wants\",\"wasnt\",\"way\",\"wed\",\"welcome\",\"went\",\"werent\",\"whatever\",\"what'll\",\"whats\",\n",
    "\"whence\",\"whenever\",\"whereafter\",\"whereas\",\"whereby\",\"wherein\",\"wheres\",\"whereupon\",\"wherever\",\"whether\",\"whim\",\n",
    "\"whither\",\"whod\",\"whoever\",\"whole\",\"who'll\",\"whomever\",\"whos\",\"whose\",\"widely\",\"willing\",\"wish\",\"within\",\"without\",\n",
    "\"wont\",\"words\",\"world\",\"wouldnt\",\"www\",\"x\",\"yes\",\"yet\",\"youd\",\"youre\",\"z\",\"zero\",\"a's\",\"ain't\",\"allow\",\"allows\",\n",
    "\"apart\",\"appear\",\"appreciate\",\"appropriate\",\"associated\",\"best\",\"better\",\"c'mon\",\"c's\",\"cant\",\"changes\",\"clearly\",\n",
    "\"concerning\",\"consequently\",\"consider\",\"considering\",\"corresponding\",\"course\",\"currently\",\"definitely\",\"described\",\n",
    "\"despite\",\"entirely\",\"exactly\",\"example\",\"going\",\"greetings\",\"hello\",\"help\",\"hopefully\",\"ignored\",\"inasmuch\",\n",
    "\"indicate\",\"indicated\",\"indicates\",\"inner\",\"insofar\",\"it'd\",\"keep\",\"keeps\",\"novel\",\"presumably\",\"reasonably\",\n",
    "\"second\",\"secondly\",\"sensible\",\"serious\",\"seriously\",\"sure\",\"t's\",\"third\",\"thorough\",\"thoroughly\",\"three\",\"well\",\"wonder\"\n",
    "]\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    text = nltk.word_tokenize(str(text).encode('ascii',errors='ignore').decode())\n",
    "    text = [token.lower() for token in text]\n",
    "    words = [word for word in text if word.isalpha() and word not in my_stopwords and len(word) > 1]\n",
    "    tags = nltk.pos_tag(words)\n",
    "    pos_tagged = [p[0]+p[1] for p in tags]\n",
    "    for token in pos_tagged:\n",
    "        result.append(token)\n",
    "    return result\n",
    "\n",
    "processed_docs = documents['content'].map(preprocess)\n",
    "bigram = gensim.models.Phrases(processed_docs, min_count=1, threshold=1)\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram = gensim.models.Phrases(bigram[processed_docs], threshold=1) \n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "trigram_docs = list()\n",
    "for processed_doc in processed_docs:\n",
    "    trigram_docs.append(trigram_mod[bigram_mod[processed_doc]])\n",
    "\n",
    "dictionary = gensim.corpora.Dictionary(trigram_docs)\n",
    "dictionary.filter_extremes(no_below=5, no_above=0.5, keep_n=5000)\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in trigram_docs]\n",
    "\n",
    "from gensim import corpora, models\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]\n",
    "#lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary, passes=2, workers=2)\n",
    "\n",
    "#print('\\nBag of Words Model:\\n')\n",
    "#for idx, topic in lda_model.print_topics(-1):\n",
    "#    print('Topic: {} \\nWords: {}\\n'.format(idx, topic))\n",
    "    \n",
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=7,minimum_probability=0.01, minimum_phi_value=0.01,\n",
    "                                             id2word=dictionary, alpha='symmetric',per_word_topics=True,passes=5, workers=5)\n",
    "\n",
    "print('\\nTF-IDF Model:\\n')\n",
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}\\n'.format(idx, topic))\n",
    "    \n",
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model_tfidf.log_perplexity(corpus_tfidf))  \n",
    "# a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = gensim.models.CoherenceModel(model=lda_model_tfidf, texts=trigram_docs, dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
