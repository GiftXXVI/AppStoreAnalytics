{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pygments import highlight\n",
    "from pygments.lexers import JsonLexer\n",
    "from pygments.formatters import TerminalFormatter\n",
    "\n",
    "from google_play_scraper import Sort, reviews, app\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
    "\n",
    "app_packages = [\n",
    "    'com.google.android.apps.meetings',\n",
    "    'com.instagram.android',\n",
    "    'com.microsoft.teams',\n",
    "    'com.zhiliaoapp.musically',\n",
    "    'com.whatsapp',\n",
    "    'us.zoom.videomeetings'\n",
    "]\n",
    "\n",
    "app_infos = []\n",
    "\n",
    "for ap in tqdm(app_packages):\n",
    "  info = app(ap, lang='en', country='us')\n",
    "  del info['comments']\n",
    "  app_infos.append(info)    \n",
    "\n",
    "app_infos_df = pd.DataFrame(app_infos)\n",
    "app_infos_df.to_csv('apps.csv', index=None, header=True)\n",
    "\n",
    "app_reviews = []\n",
    "\n",
    "for ap in tqdm(app_packages):\n",
    "  for score in list(range(1, 6)):\n",
    "  #for x in list(range(1, 10)):\n",
    "    for sort_order in [Sort.NEWEST]:\n",
    "      rvs, _ = reviews(\n",
    "        ap,\n",
    "        lang='en',\n",
    "        country='us',\n",
    "        sort=sort_order,\n",
    "        count= 10000,\n",
    "        filter_score_with=score\n",
    "        #filter_score_with=None\n",
    "      )\n",
    "      for r in rvs:\n",
    "        r['sortOrder'] = 'most_relevant' if sort_order == Sort.MOST_RELEVANT else 'newest'\n",
    "        r['appId'] = ap\n",
    "      app_reviews.extend(rvs)\n",
    "\n",
    "app_reviews_df = pd.DataFrame(app_reviews)\n",
    "app_reviews_df.to_csv('reviews.csv', index=None, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bag of Words Model:\n",
      "\n",
      "Topic: 0 \n",
      "Words: 0.127*\"meeting\" + 0.104*\"super\" + 0.023*\"good\" + 0.018*\"download\" + 0.017*\"superb\" + 0.014*\"happy\" + 0.011*\"class\" + 0.010*\"features\" + 0.010*\"working_properly\" + 0.007*\"free\"\n",
      "\n",
      "Topic: 1 \n",
      "Words: 0.067*\"excellent\" + 0.038*\"awesome\" + 0.024*\"virtual_background\" + 0.019*\"online_classes\" + 0.019*\"thanks\" + 0.015*\"poor\" + 0.015*\"quality\" + 0.015*\"video_quality\" + 0.014*\"need\" + 0.013*\"sound\"\n",
      "\n",
      "Topic: 2 \n",
      "Words: 0.085*\"zoom\" + 0.045*\"time\" + 0.020*\"working\" + 0.015*\"problem\" + 0.011*\"fantastic\" + 0.010*\"connection\" + 0.007*\"going\" + 0.007*\"usefull\" + 0.006*\"work\" + 0.006*\"like\"\n",
      "\n",
      "Topic: 3 \n",
      "Words: 0.551*\"nice\" + 0.060*\"like\" + 0.054*\"love\" + 0.042*\"helpful\" + 0.017*\"perfect\" + 0.007*\"students\" + 0.006*\"nice_online_classes\" + 0.005*\"meetings\" + 0.004*\"boring\" + 0.004*\"loved\"\n",
      "\n",
      "Topic: 4 \n",
      "Words: 0.105*\"great\" + 0.061*\"useful\" + 0.036*\"work\" + 0.026*\"thank\" + 0.025*\"online_class\" + 0.022*\"school\" + 0.020*\"wonderful\" + 0.019*\"sign\" + 0.014*\"best_meeting\" + 0.013*\"network_problem\"\n",
      "\n",
      "Topic: 5 \n",
      "Words: 0.113*\"best\" + 0.038*\"class\" + 0.015*\"nice_meeting\" + 0.014*\"clear\" + 0.013*\"help\" + 0.013*\"works\" + 0.012*\"zoom_meeting\" + 0.012*\"meet\" + 0.012*\"zoom\" + 0.010*\"good_online_classes\"\n",
      "\n",
      "Topic: 6 \n",
      "Words: 0.053*\"problem\" + 0.044*\"update\" + 0.036*\"video\" + 0.020*\"zoom\" + 0.015*\"like\" + 0.015*\"easy\" + 0.011*\"issues\" + 0.011*\"host\" + 0.010*\"want\" + 0.010*\"screen\"\n",
      "\n",
      "Topic: 7 \n",
      "Words: 0.063*\"worst\" + 0.044*\"better\" + 0.041*\"amazing\" + 0.034*\"classes\" + 0.022*\"problems\" + 0.019*\"waste\" + 0.018*\"audio\" + 0.017*\"times\" + 0.016*\"cool\" + 0.015*\"connect\"\n",
      "\n",
      "Topic: 8 \n",
      "Words: 0.032*\"zoom\" + 0.019*\"like\" + 0.017*\"apps\" + 0.017*\"star\" + 0.014*\"option\" + 0.014*\"meetings\" + 0.014*\"application\" + 0.013*\"experience\" + 0.012*\"know\" + 0.011*\"video\"\n",
      "\n",
      "Topic: 9 \n",
      "Words: 0.751*\"good\" + 0.013*\"update\" + 0.010*\"fine\" + 0.007*\"bugs\" + 0.006*\"problem\" + 0.003*\"helpfull\" + 0.003*\"think\" + 0.003*\"audio_problem\" + 0.002*\"help\" + 0.002*\"conference\"\n",
      "\n",
      "\n",
      "TF-IDF Model:\n",
      "\n",
      "Topic: 0 \n",
      "Words: 0.084*\"worst\" + 0.022*\"online_class\" + 0.021*\"experience\" + 0.020*\"poor\" + 0.019*\"thank\" + 0.016*\"okay\" + 0.014*\"audio\" + 0.013*\"good_meetings\" + 0.011*\"good\" + 0.008*\"zoom_best\"\n",
      "\n",
      "Topic: 1 \n",
      "Words: 0.063*\"useful\" + 0.038*\"meeting\" + 0.016*\"fine\" + 0.014*\"happy\" + 0.013*\"students\" + 0.011*\"bugs\" + 0.010*\"want\" + 0.010*\"good\" + 0.010*\"working_properly\" + 0.009*\"good_online_class\"\n",
      "\n",
      "Topic: 2 \n",
      "Words: 0.049*\"better\" + 0.034*\"class\" + 0.021*\"best_meeting\" + 0.020*\"sign\" + 0.015*\"meetings\" + 0.013*\"quality\" + 0.013*\"star\" + 0.011*\"good_experience\" + 0.010*\"zoom\" + 0.010*\"best_online_classes\"\n",
      "\n",
      "Topic: 3 \n",
      "Words: 0.694*\"good\" + 0.100*\"super\" + 0.015*\"easy\" + 0.015*\"fantastic\" + 0.008*\"classes\" + 0.007*\"slow\" + 0.006*\"nice_online_classes\" + 0.006*\"mast\" + 0.006*\"nice_application\" + 0.005*\"beautiful\"\n",
      "\n",
      "Topic: 4 \n",
      "Words: 0.062*\"like\" + 0.046*\"amazing\" + 0.018*\"wonderful\" + 0.018*\"problem\" + 0.014*\"help\" + 0.010*\"good\" + 0.010*\"improve\" + 0.009*\"connect\" + 0.008*\"meeting\" + 0.008*\"connecting\"\n",
      "\n",
      "Topic: 5 \n",
      "Words: 0.064*\"zoom\" + 0.020*\"online_classes\" + 0.018*\"thanks\" + 0.015*\"data\" + 0.011*\"voice_clear\" + 0.010*\"good\" + 0.010*\"need\" + 0.009*\"phone\" + 0.008*\"mobile\" + 0.008*\"open\"\n",
      "\n",
      "Topic: 6 \n",
      "Words: 0.112*\"excellent\" + 0.062*\"awesome\" + 0.053*\"helpful\" + 0.033*\"waste\" + 0.029*\"cool\" + 0.021*\"download\" + 0.016*\"apps\" + 0.016*\"good_online_classes\" + 0.014*\"usefull\" + 0.013*\"virtual_background\"\n",
      "\n",
      "Topic: 7 \n",
      "Words: 0.121*\"best\" + 0.092*\"great\" + 0.020*\"nice_meeting\" + 0.018*\"work\" + 0.013*\"clear\" + 0.010*\"good_application\" + 0.008*\"good\" + 0.007*\"user_friendly\" + 0.007*\"audio_problem\" + 0.006*\"student\"\n",
      "\n",
      "Topic: 8 \n",
      "Words: 0.054*\"love\" + 0.025*\"update\" + 0.020*\"video\" + 0.015*\"perfect\" + 0.014*\"problems\" + 0.013*\"sound\" + 0.013*\"application\" + 0.012*\"time\" + 0.012*\"good\" + 0.012*\"hate\"\n",
      "\n",
      "Topic: 9 \n",
      "Words: 0.622*\"nice\" + 0.019*\"superb\" + 0.015*\"working\" + 0.013*\"school\" + 0.010*\"network_problem\" + 0.010*\"important\" + 0.009*\"works\" + 0.007*\"love_zoom\" + 0.006*\"supper\" + 0.006*\"good_online\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "data = pd.read_csv(\"reviews_zoom_v2.csv\")\n",
    "data_text = data[['content']]\n",
    "data_text['index'] = data_text.index\n",
    "documents = data_text\n",
    "\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np\n",
    "\n",
    "def lemmatize_only(text):\n",
    "    return WordNetLemmatizer().lemmatize(text, pos='v')\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(str(text).encode('ascii',errors='ignore').decode()):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(token)\n",
    "        #result.append(token)\n",
    "    return result\n",
    "\n",
    "processed_docs = documents['content'].map(preprocess)\n",
    "bigram = gensim.models.Phrases(processed_docs, min_count=1, threshold=1)\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram = gensim.models.Phrases(bigram[processed_docs], threshold=1) \n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "trigram_docs = list()\n",
    "for processed_doc in processed_docs:\n",
    "    trigram_docs.append(trigram_mod[bigram_mod[processed_doc]])\n",
    "\n",
    "dictionary = gensim.corpora.Dictionary(trigram_docs)\n",
    "dictionary.filter_extremes(no_below=5, no_above=0.5, keep_n=5000)\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in trigram_docs]\n",
    "\n",
    "from gensim import corpora, models\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]\n",
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary, passes=10, workers=10)\n",
    "\n",
    "print('\\nBag of Words Model:\\n')\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}\\n'.format(idx, topic))\n",
    "    \n",
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=10, id2word=dictionary, passes=10, workers=10)\n",
    "\n",
    "print('\\nTF-IDF Model:\\n')\n",
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}\\n'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
